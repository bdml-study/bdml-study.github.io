I"<p>ì´ë²ˆì‹œê°„ì—ëŠ” <strong>Sherlock: A Deep Learning Approach to Semantic Data Type Detection</strong> ë…¼ë¬¸ì„ ë¦¬ë·°í•´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤.</p>

<h2 id="abstract">Abstract</h2>

<p>ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ìœ„í•œ self-supervised learning ë°©ë²•ì—ëŠ” í¬ê²Œ 3ê°€ì§€ ì¢…ë¥˜ë¡œ ë‚˜ë‰œë‹¤.</p>

<ul>
  <li>Contrastive learning</li>
  <li>Object-level representation learning</li>
  <li>Masked modeling</li>
</ul>

<p>ì´ë²ˆ ì‹œê°„ì—ëŠ” Contrastive learningì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.</p>

<!--more-->

<h3 id="contrastive-learning-ì´ë€">Contrastive Learning ì´ë€?</h3>

<p>ë°ì´í„°ê³µê°„ì—ì„œ latent ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë§¤í•‘í•¨ìˆ˜($\mathcal{X} \rightarrow \mathcal{Z}$)ë¥¼ í•™ìŠµí•˜ëŠ” í•™ìŠµ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ, ë³„ë„ì˜ labelì´ í•„ìš”í•˜ì§€ ì•Šì€ self supervised learning ë°©ë²•ì´ë‹¤.</p>

<p>ì–´ë–¤ ë°ì´í„° ì§‘í•©ì´ ìˆê³ , í•œ ë°ì´í„° ìƒ˜í”Œì´ ìˆì„ ë•Œ, ìœ ì‚¬í•´ì•¼ í•˜ëŠ” ìƒ˜í”Œê³¼ëŠ” latent ê³µê°„ì—ì„œ ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ, ë‹¬ë¼ì•¼ í•˜ëŠ” ìƒ˜í”Œê³¼ì˜ ê±°ë¦¬ëŠ” ë©€ê²Œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë§¤í•‘í•¨ìˆ˜ë¥¼ í•™ìŠµí•œë‹¤.</p>

<p>Contrastive learning lossëŠ” ë³´í†µ ë‹¤ìŒê³¼ ê°™ì´ ìƒê²¼ë‹¤.</p>

<ol>
  <li>
    <p>ì§„ì§œ ê±°ë¦¬ë¥¼ ë¹„êµí•˜ëŠ” í˜•íƒœ<br />
 ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ì‹ì„ ë°”ë¡œ loss functionì— ì ìš©í•˜ì—¬ anchorì™€ positiveëŠ” ê°€ê¹ê²Œ, anchorì™€ negativeì™€ëŠ” ë©€ê²Œ ë§Œë“ ë‹¤. (ì˜ˆì‹œ: triplet loss)</p>

\[L = \text{dist}(x_{anc}, x_{pos}) - \text{dist}(x_{anc}, x_{neg})\]
  </li>
  <li>
    <p>NCE(Noise Contrastive Estimation) í˜•íƒœ<br />
 NLPì˜ negative samplingì²˜ëŸ¼ negative sampleì„ ì—¬ëŸ¬ê°œ ë‘ê³  ë¶„ìë¥¼ minimize, ë¶„ëª¨ë¥¼ maximizeí•˜ë©´ anchorì™€ positiveëŠ” ê°€ê¹ê²Œ, anchorì™€ negativeì™€ëŠ” ë©€ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. (ì˜ˆì‹œ: InfoNCE)</p>

\[L = \mathbb{E}_{x_{anc}}\left[ -\log \frac{\exp \left( \text{sim}(x_{anc}, x_{pos}) \right)}{\sum_{x_{neg}} \exp \left( \text{sim}(x_{anc}, x_{neg}) \right)} \right]\]

    <p>$\text{sim}(a, b)$ì˜ ê²½ìš°, ë³´í†µ ë‚´ì  ë˜ëŠ” cosine similarityë¥¼ ë§ì´ ì“´ë‹¤.</p>
  </li>
</ol>

<h3 id="contrastive-learningì˜-í•™ìŠµë°©ë²•">Contrastive Learningì˜ í•™ìŠµë°©ë²•</h3>

<p>Contrastive learningì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ í•™ìŠµëœë‹¤.</p>

<ol>
  <li>ì´ë¯¸ì§€ì—ì„œ íŒ¨ì¹˜ $x$ë¥¼ ë½‘ëŠ”ë‹¤.</li>
  <li>íŒ¨ì¹˜ $x$ë¥¼ augmentationí•´ì„œ íŒ¨ì¹˜ $\tilde{x}$ë¥¼ ì–»ëŠ”ë‹¤.</li>
  <li>í•´ë‹¹ íŒ¨ì¹˜ ì´ì™¸ì˜ ì˜ì—­ì—ì„œ ë‹¤ë¥¸ íŒ¨ì¹˜ $y$ë¥¼ ë½‘ëŠ”ë‹¤.</li>
  <li>$x$ê°€ anchor, $\tilde{x}$ê°€ positive, $y$ê°€ negative ìƒ˜í”Œì´ ë˜ì–´, contrastive learningì„ í†µí•´ ì¸ì½”ë”ë¥¼ í•™ìŠµí•œë‹¤.</li>
</ol>

<p>contrastive learningì€ ë³´í†µ negative sampleì˜ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì˜ ë™ì‘í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ í•œ ì´ë¯¸ì§€ì—ì„œ anchor, positive, negative ìƒ˜í”Œì„ ëª¨ë‘ ë½‘ê²Œ ë˜ë©°, anchor íŒ¨ì¹˜ ì´ì™¸ì˜ ì˜ì—­ì€ <strong>ëª¨ë‘ negetive íŒ¨ì¹˜</strong>ê°€ ëœë‹¤.</p>

<p>ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì€ ì‘ì€ í¬ê¸°ì˜ ë°°ì¹˜ì—ì„œë„ ë§ì€ ì–‘ì˜ negative sampleì„ í™•ë³´í•  ìˆ˜ ìˆê²Œ í•œë‹¤.</p>

<p>ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ contrastive learningì€ ë‹¤ìŒ ë…¼ë¬¸ì´ë‹¤.(SimCLR, 2020)</p>

<p><a href="https://www.notion.so/A-Simple-Framwork-for-Contrastive-Learning-of-Visual-Representation-09835248af6f4a23bfe16045dab8ed84">A Simple Framwork for Contrastive Learning of Visual Representation</a></p>

<p>ê·¸ëŸ°ë° ì´ ë…¼ë¬¸ì˜ ê²½ìš° negative sampleì„ í•˜ë‚˜ì˜ ë°°ì¹˜ ë‚´ì— ìˆëŠ” ì• ë“¤ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, negative sample í­ì´ ë„ˆë¬´ ì¢ë‹¤.</p>

<p>ë‹¤ìŒ ë…¼ë¬¸ì€ ë©”ëª¨ë¦¬ ë±…í¬ë¥¼ ì´ìš©í•˜ì—¬ ì´ì „ì— í•™ìŠµí•´ë‘” ìƒ˜í”Œì„ queueì— ì €ì¥í•´ë‘ê³  negative sampleë¡œ í™œìš©í•œë‹¤. (MoCo, 2019)</p>

<p><a href="https://www.notion.so/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning-7445c72ab3db40b7b3420771e59e5c32">Momentum Contrast for Unsupervised Visual Representation Learning</a></p>

<h3 id="contrast-without-negative-samples">Contrast Without Negative Samples</h3>

<p>ìœ„ ë‘ ë…¼ë¬¸ì˜ ë¬¸ì œì ì€ ëª¨ë‘ negative sampleì„ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ìƒê¸´ ë¬¸ì œì´ë‹¤.</p>

<p>ì•„ë˜ ë…¼ë¬¸ë“¤ì€ negative sample ì—†ì´ positive sampleë§Œ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê¸°ë²•ì— ëŒ€í•œ ë…¼ë¬¸ì´ë‹¤.</p>

<p>ë‹¤ë§Œ, positive sampleë§Œ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”ë¥¼ í•™ìŠµí•  ê²½ìš° ëª¨ë“  ë°ì´í„°ì˜ latentê°€ ê°™ì•„ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤.</p>

<p>ëª¨ë“  ë°ì´í„°ì˜ latent ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ë©´, lossê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<p>ì´ë¥¼ ë§‰ì„ ê¸°ë²•ì´ ì•„ë˜ ë…¼ë¬¸ë“¤ì˜ í•µì‹¬ì´ë©°, ê°ìì˜ ì°¨ì´ì ì´ë‹¤.</p>

<p>(2020)</p>

<p><a href="https://www.notion.so/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning-cf940bd163c448f4a9fc26cd1a917bc0">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</a></p>

<p>(2021)</p>

<p><a href="https://www.notion.so/Exploring-Simple-Siamese-Representation-Learning-f68fac6af1004534bdfe2adda7314f32">Exploring Simple Siamese Representation Learning</a></p>

<p><a href="https://www.notion.so/Barlow-Twins-Self-Supervised-Learning-via-Redundancy-Reduction-77d1f829c1a0417bac5ef48214fa7f73">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a></p>

<p>Contrastive learning ìµœì‹  ë…¼ë¬¸ë“¤ ë³´ë©´, ê±°ì˜ ëª¨ë“  ë…¼ë¬¸ë“¤ì´ latentê°€ í•˜ë‚˜ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë§‰ëŠ” ê²ƒì— ëŒ€í•œ ì—°êµ¬ê°€ ëŒ€ë¶€ë¶„ì¸ë“¯í•˜ë‹¤.</p>
:ET