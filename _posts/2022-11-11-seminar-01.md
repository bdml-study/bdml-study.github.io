---
layout: post
title: "Contrastive Learning"
description: "A work at home parent is an entrepreneur who works from home and integrates parenting into his or her business activities."
date: 2022-11-11
# feature_image: images/desk.jpg 
tags: [self-supervised learning, contrastive learning]
use_math: true
author: "이재영"
---

## Self-Supervised Learning in Computer Vision

이미지 데이터를 위한 self-supervised learning 방법에는 크게 3가지 종류로 나뉜다.

- Contrastive learning
- Object-level representation learning
- Masked modeling

이번 시간에는 Contrastive learning에 대해 알아본다.

### Contrastive Learning 이란?

데이터공간에서 latent 공간으로 매핑하는 매핑함수($\mathcal{X} \rightarrow \mathcal{Z}$)를 학습하는 학습 방법 중 하나로, 별도의 label이 필요하지 않은 self supervised learning 방법이다. 

어떤 데이터 집합이 있고, 한 데이터 샘플이 있을 때, 유사해야 하는 샘플과는 latent 공간에서 거리를 가깝게, 달라야 하는 샘플과의 거리는 멀게 학습함으로써 매핑함수를 학습한다.

Contrastive learning loss는 보통 다음과 같이 생겼다.

1. 진짜 거리를 비교하는 형태  
    거리를 구하는 식을 바로 loss function에 적용하여 anchor와 positive는 가깝게, anchor와 negative와는 멀게 만든다. (예시: triplet loss)

    $$
    L = \text{dist}(x_{anc}, x_{pos}) - \text{dist}(x_{anc}, x_{neg})
    $$

2. NCE(Noise Contrastive Estimation) 형태  
    NLP의 negative sampling처럼 negative sample을 여러개 두고 분자를 minimize, 분모를 maximize하면 anchor와 positive는 가깝게, anchor와 negative와는 멀게 만들 수 있다. (예시: InfoNCE)

    $$
    L = \mathbb{E}_{x_{anc}}\left[ -\log \frac{\exp \left( \text{sim}(x_{anc}, x_{pos}) \right)}{\sum_{x_{neg}} \exp \left( \text{sim}(x_{anc}, x_{neg}) \right)} \right]
    $$

    $\text{sim}(a, b)$의 경우, 보통 내적 또는 cosine similarity를 많이 쓴다.

### Contrastive Learning의 학습방법

Contrastive learning은 다음과 같은 과정으로 학습된다.

1. 이미지에서 패치 $x$를 뽑는다.
2. 패치 $x$를 augmentation해서 패치 $\tilde{x}$를 얻는다.
3. 해당 패치 이외의 영역에서 다른 패치 $y$를 뽑는다.
4. $x$가 anchor, $\tilde{x}$가 positive, $y$가 negative 샘플이 되어, contrastive learning을 통해 인코더를 학습한다.

contrastive learning은 보통 negative sample의 수가 많을수록 잘 동작한다. 

그래서 한 이미지에서 anchor, positive, negative 샘플을 모두 뽑게 되며, anchor 패치 이외의 영역은 **모두 negetive 패치**가 된다. 

이러한 학습 과정은 작은 크기의 배치에서도 많은 양의 negative sample을 확보할 수 있게 한다.

가장 기본적인 형태의 contrastive learning은 다음 논문이다.(SimCLR, 2020)

[A Simple Framwork for Contrastive Learning of Visual Representation](https://www.notion.so/A-Simple-Framwork-for-Contrastive-Learning-of-Visual-Representation-09835248af6f4a23bfe16045dab8ed84)

그런데 이 논문의 경우 negative sample을 하나의 배치 내에 있는 애들만 사용하므로, negative sample 폭이 너무 좁다.

다음 논문은 메모리 뱅크를 이용하여 이전에 학습해둔 샘플을 queue에 저장해두고 negative sample로 활용한다. (MoCo, 2019)

[Momentum Contrast for Unsupervised Visual Representation Learning](https://www.notion.so/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning-7445c72ab3db40b7b3420771e59e5c32)

### Contrast Without Negative Samples

위 두 논문의 문제점은 모두 negative sample을 사용해야 하기 때문에 생긴 문제이다. 

아래 논문들은 negative sample 없이 positive sample만 이용하여 학습하는 기법에 대한 논문이다. 

다만, positive sample만 사용하여 인코더를 학습할 경우 모든 데이터의 latent가 같아지는 현상이 발생한다. 

모든 데이터의 latent 사이의 거리를 0으로 만들면, loss가 0이 되기 때문이다. 

이를 막을 기법이 아래 논문들의 핵심이며, 각자의 차이점이다.

(2020)

[Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://www.notion.so/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning-cf940bd163c448f4a9fc26cd1a917bc0)

(2021)

[Exploring Simple Siamese Representation Learning](https://www.notion.so/Exploring-Simple-Siamese-Representation-Learning-f68fac6af1004534bdfe2adda7314f32)

[Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://www.notion.so/Barlow-Twins-Self-Supervised-Learning-via-Redundancy-Reduction-77d1f829c1a0417bac5ef48214fa7f73)

Contrastive learning 최신 논문들 보면, 거의 모든 논문들이 latent가 하나로 수렴하는 것을 막는 것에 대한 연구가 대부분인듯하다.  
