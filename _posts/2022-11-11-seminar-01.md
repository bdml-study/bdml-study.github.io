---
layout: post
title: "Contrastive Learning"
# description: "Contrastive Learning 이란 별도의 label이 필요하지 않은 self supervised learning 방법이다."
date: 2022-11-11
# feature_image: images/desk.jpg 
tags: [self-supervised learning, contrastive learning]
use_math: true
author: "이재영"
---

## Self-Supervised Learning in Computer Vision

이미지 데이터를 위한 self-supervised learning 방법에는 크게 3가지 종류로 나뉜다.

- Contrastive learning
- Object-level representation learning
- Masked modeling

이번 시간에는 Contrastive learning에 대해 알아본다.

<!--more-->

### Contrastive Learning 이란?

데이터공간에서 latent 공간으로 매핑하는 매핑함수($\mathcal{X} \rightarrow \mathcal{Z}$)를 학습하는 학습 방법 중 하나로, 별도의 label이 필요하지 않은 self supervised learning 방법이다. 

어떤 데이터 집합이 있고, 한 데이터 샘플이 있을 때, 유사해야 하는 샘플과는 latent 공간에서 거리를 가깝게, 달라야 하는 샘플과의 거리는 멀게 학습함으로써 매핑함수를 학습한다.

Contrastive learning loss는 보통 다음과 같이 생겼다.

1. 진짜 거리를 비교하는 형태  
    거리를 구하는 식을 바로 loss function에 적용하여 anchor와 positive는 가깝게, anchor와 negative와는 멀게 만든다. (예시: triplet loss)

    $$
    L = \text{dist}(x_{anc}, x_{pos}) - \text{dist}(x_{anc}, x_{neg})
    $$

2. NCE(Noise Contrastive Estimation) 형태  
    NLP의 negative sampling처럼 negative sample을 여러개 두고 분자를 minimize, 분모를 maximize하면 anchor와 positive는 가깝게, anchor와 negative와는 멀게 만들 수 있다. (예시: InfoNCE)

    $$
    L = \mathbb{E}_{x_{anc}}\left[ -\log \frac{\exp \left( \text{sim}(x_{anc}, x_{pos}) \right)}{\sum_{x_{neg}} \exp \left( \text{sim}(x_{anc}, x_{neg}) \right)} \right]
    $$

    $\text{sim}(a, b)$의 경우, 보통 내적 또는 cosine similarity를 많이 쓰며, 거리의 음의 값을 쓰기도 한다.

### Contrastive Learning의 학습방법

Contrastive learning은 다음과 같은 과정으로 학습된다.

1. 이미지에서 패치 $x$를 뽑는다.
2. 패치 $x$를 augmentation해서 패치 $\tilde{x}$를 얻는다.
3. 해당 패치 이외의 영역에서 다른 패치 $y$를 뽑는다.
4. $x$가 anchor, $\tilde{x}$가 positive, $y$가 negative 샘플이 되어, contrastive learning을 통해 인코더를 학습한다.

contrastive learning은 보통 negative sample의 수가 많을수록 잘 동작한다. 

그래서 한 이미지에서 anchor, positive, negative 샘플을 모두 뽑게 되며, anchor 패치 이외의 영역은 **모두 negetive 패치**가 된다. 

이러한 학습 과정은 작은 크기의 배치에서도 많은 양의 negative sample을 확보할 수 있게 한다.

가장 기본적인 형태의 contrastive learning은 다음 논문이다.(SimCLR, 2020)

[A Simple Framwork for Contrastive Learning of Visual Representation](https://www.notion.so/A-Simple-Framwork-for-Contrastive-Learning-of-Visual-Representation-09835248af6f4a23bfe16045dab8ed84)

그런데 이 논문의 경우 negative sample을 하나의 배치 내에 있는 애들만 사용하므로, negative sample 폭이 너무 좁다.

다음 논문은 메모리 뱅크를 이용하여 이전에 학습해둔 샘플을 queue에 저장해두고 negative sample로 활용한다. (MoCo, 2019)

[Momentum Contrast for Unsupervised Visual Representation Learning](https://www.notion.so/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning-7445c72ab3db40b7b3420771e59e5c32)

### Contrast Without Negative Samples

위 두 논문의 문제점은 모두 negative sample을 사용해야 하기 때문에 생긴 문제이다. 

아래 논문들은 negative sample 없이 positive sample만 이용하여 학습하는 기법에 대한 논문이다. 

다만, positive sample만 사용하여 인코더를 학습할 경우 모든 데이터의 latent가 같아지는 현상이 발생한다. 

모든 데이터의 latent 사이의 거리를 0으로 만들면, loss가 0이 되기 때문이다. 

이를 막을 기법이 아래 논문들의 핵심이며, 각자의 차이점이다.

(2020)

[Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://www.notion.so/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning-cf940bd163c448f4a9fc26cd1a917bc0)

(2021)

[Exploring Simple Siamese Representation Learning](https://www.notion.so/Exploring-Simple-Siamese-Representation-Learning-f68fac6af1004534bdfe2adda7314f32)

[Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://www.notion.so/Barlow-Twins-Self-Supervised-Learning-via-Redundancy-Reduction-77d1f829c1a0417bac5ef48214fa7f73)

Contrastive learning 최신 논문들 보면, 거의 모든 논문들이 latent가 하나로 수렴하는 것을 막는 것에 대한 연구가 대부분인듯하다.  



## Object-Centric Representation Learning

Contrastive learning은 이미지 데이터가 모두 object-centric이라고 가정한다. 즉, 이미지는 모두 관심있는 객체를 중심으로 캡쳐되었다고 가정한다. 하지만, 이 가정을 만족시키지 않는 이미지 데이터도 많다. 본 카테고리는 이미지가 모두 object-centric이라고 가정하는 contrastive learning의 문제점을 해결하기 위한 self supervised learning 방법을 다루며, 이미지에서 object의 위치(관심 영역 위치)를 스스로 찾아서 해당 부분을 중심으로 representation을 학습하게 하는 기법들이다.

****Unsupervised Object-Level Representation Learning from Scene Images (NeurIPS 2021)****

****Object discovery and representation networks (ECCV 2022)****



## Masked Modeling

BERT 학습 방법 중 masked language modeling을 이미지 self-supervised learning에 적용한 기법이다.

Masked autoencoder는 masked modeling을 활용한 self supervised learning 논문 중 가장 유명한(?) 논문이라고 볼 수 있다. 본 논문은 트랜스포머 구조의 인코더와 디코더를 사용하는데, 입력 이미지의 몇 개의 패치를 지운 후, 멀쩡한 패치만 인코더에 넣어서 인코딩하고, 디코더가 모든 패치를 복원하도록 학습한다. 이 과정에서 인코더는 지워진 패치의 정보 또한 예측할 수 있는 latent를 생성할 수 있게 된다. (2022)

[Masked Autoencoders are Scalable Vision Learner](https://www.notion.so/Masked-Autoencoders-are-Scalable-Vision-Learner-424011fc9d9948568539f74c8884f40d)

BEiT는 masked autoencoder와 유사하지만, 패치를 직접 예측(생성)하지 않고, 패치의 id를 예측하게 하는 인코더이다. 이 논문은 패치를 id로 매핑하기 위해 discrete VAE(dVAE)를 사용한다. (2022)

****BEiT: BERT Pre-Training of Image Transformers****
